{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Workshop on TFX - Components, Pipeline and Serving\n",
    "\n",
    "TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines\n",
    "\n",
    "A TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. Components are built using TFX libraries which can also be used individually.\n",
    "\n",
    "\n",
    "\n",
    "In this workshop, we will build a simple Image Classification TFX pipeline. After the workshop, one should be able to:\n",
    "1. Have a basic understanding of the role of TFX pipelines.\n",
    "2. Have a basic understanding of Transform, Trainer and Push TFX components.\n",
    "3. Have an idea to convert their ML training code to a TFX pipeline and run it in Local or Apache AirFlow.\n",
    "4. Have an idea of how to deploy models using Tensorflow Serving.\n",
    "\n",
    "This workshop is divided into three sections:\n",
    "1. TFX: Understanding basic TFX component using interactive environment.\n",
    "2. TFX Pipeline Orchestration using: Local and Apache Airflow (optional).\n",
    "3. Model deployment: Flask and Tensorflow Serving\n",
    "\n",
    "Reference: https://www.tensorflow.org/tfx"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to setup TFX on Google Colab\r\n",
    "try:\r\n",
    "  import colab\r\n",
    "  !pip install --upgrade pip\r\n",
    "  !pip install -U tfx\r\n",
    "except:\r\n",
    "  pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: In Google Colab, because of package updates, the first time you run this cell you must restart the runtime (Runtime > Restart runtime ...).**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFX Components: Interactive Orchestration\n",
    "\n",
    "### Background\n",
    "This notebook demonstrates how to use TFX in a Jupyter/Colab environment. Here, we walk through the Chicago Taxi example in an interactive notebook.\n",
    "\n",
    "Working in an interactive notebook is a useful way to become familiar with the structure of a TFX pipeline. It's also useful when doing development of your own pipelines as a lightweight development environment, but you should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.\n",
    "\n",
    "### Orchestration\n",
    "In a production deployment of TFX, you will use an orchestrator such as Apache Airflow, Kubeflow Pipelines, or Apache Beam to orchestrate a pre-defined pipeline graph of TFX components. In an interactive notebook, the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.\n",
    "\n",
    "### Metadata\n",
    "In a production deployment of TFX, you will access metadata through the ML Metadata (MLMD) API. MLMD stores metadata properties in a database such as MySQL or SQLite, and stores the metadata payloads in a persistent store such as on your filesystem. In an interactive notebook, both properties and payloads are stored in an ephemeral SQLite database in the /tmp directory on the Jupyter notebook or Colab server.\n",
    "\n",
    "Adapted from: https://www.tensorflow.org/tfx/tutorials/tfx/components_keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./assets/tfx_workflow.png\" alt=\"TFX Component\" height=\"1080\" width=\"1900px\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./assets/tfx_libraries.png\" alt=\"TFX Libraries\" height=\"900\" width=\"1900px\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import tensorflow as tf\r\n",
    "from typing import List\r\n",
    "from tfx import v1 as tfx\r\n",
    "\r\n",
    "from tfx.components import ImportExampleGen\r\n",
    "from tfx.components import Pusher\r\n",
    "from tfx.components import SchemaGen\r\n",
    "from tfx.components import StatisticsGen\r\n",
    "from tfx.components import Trainer\r\n",
    "from tfx.components import Transform\r\n",
    "from tfx.orchestration import metadata\r\n",
    "from tfx.orchestration import pipeline\r\n",
    "from tfx.proto import example_gen_pb2\r\n",
    "from tfx.proto import pusher_pb2\r\n",
    "from tfx.proto import trainer_pb2\r\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check tensorflow and TFX versions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))\r\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup the pipeline paths for artifacts, metadata, and output model generated to be used later"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_pipeline_name = \"cifar10-tfx\"\r\n",
    "\r\n",
    "_pipeline_root = os.path.join('pipelines', _pipeline_name)\r\n",
    "_data_root = \"data\"\r\n",
    "_serving_model_dir = os.path.join('serving_model', _pipeline_name)\r\n",
    "_labels_path = os.path.join(_data_root, 'labels.txt')\r\n",
    "\r\n",
    "_trainer_module_file = \"cifar10_trainer.py\"\r\n",
    "_transform_module_file = \"cifar10_transform.py\"\r\n",
    "\r\n",
    "# Will be used later for TFX pipeline\r\n",
    "_metadata_path = os.path.join('metadata', _pipeline_name, 'metadata.db')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an interactive context"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "context = InteractiveContext()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Components"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ImportGen Component\n",
    "\n",
    "The ImportExampleGen component takes TFRecord files with TF Example data format, and generates train and eval examples for downstream components. This component provides consistent and configurable partition, and it also shuffle the dataset for ML best practice.\n",
    "\n",
    "More details: https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/ImportExampleGen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_config = example_gen_pb2.Input(splits=[\r\n",
    "    example_gen_pb2.Input.Split(name='train', pattern='train/*'),\r\n",
    "    example_gen_pb2.Input.Split(name='eval', pattern='test/*')\r\n",
    "])\r\n",
    "example_gen = ImportExampleGen(input_base=_data_root,\r\n",
    "                               input_config=input_config)\r\n",
    "\r\n",
    "context.run(example_gen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "artifact = example_gen.outputs['examples'].get()[0]\r\n",
    "print(artifact.split_names, artifact.uri)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### StatisticsGen Component\n",
    "\n",
    "The StatisticsGen TFX pipeline component generates features statistics over both training and serving data, which can be used by other pipeline components. StatisticsGen uses Beam to scale to large datasets.\n",
    "\n",
    "Consumes: datasets created by an ExampleGen pipeline component.\n",
    "\n",
    "Emits: Dataset statistics.\n",
    "\n",
    "More details: https://www.tensorflow.org/tfx/guide/statsgen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\r\n",
    "\r\n",
    "context.run(statistics_gen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SchemaGen Component"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'],\r\n",
    "                           infer_feature_shape=True)\r\n",
    "\r\n",
    "context.run(schema_gen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform Component\n",
    "\n",
    "* Consumes: tf.Examples from an ExampleGen component, and a data schema from a SchemaGen component.\n",
    "* Emits: A SavedModel to a Trainer component, pre-transform and post-transform statistics.\n",
    "\n",
    "More details: https://www.tensorflow.org/tfx/guide/transform\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transform = Transform(examples=example_gen.outputs['examples'],\r\n",
    "                      schema=schema_gen.outputs['schema'],\r\n",
    "                      module_file=_transform_module_file)\r\n",
    "\r\n",
    "context.run(transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's examine the output artifacts of Transform. This component produces two types of outputs:\n",
    "\n",
    "* transform_graph is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).\n",
    "* transformed_examples represents the preprocessed training and evaluation data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_uri = transform.outputs['transform_graph'].get()[0].uri\r\n",
    "os.listdir(train_uri)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\r\n",
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\r\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\r\n",
    "\r\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\r\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\r\n",
    "                      for name in os.listdir(train_uri)]\r\n",
    "\r\n",
    "# Create a `TFRecordDataset` to read these files\r\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\r\n",
    "\r\n",
    "# Doesn't work on Colab with default params\r\n",
    "# Iterate over the first record and decode them.\r\n",
    "for tfrecord in dataset.take(1):\r\n",
    "  serialized_example = tfrecord.numpy()\r\n",
    "  example = tf.train.Example()\r\n",
    "  example.ParseFromString(serialized_example)\r\n",
    "  print(example)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer Component\n",
    "\n",
    "The Trainer TFX pipeline component trains a TensorFlow model.\n",
    "\n",
    "Trainer takes:\n",
    "* tf.Examples used for training and eval.\n",
    "* A user provided module file that defines the trainer logic.\n",
    "* Protobuf definition of train args and eval args.\n",
    "* (Optional) A data schema created by a SchemaGen pipeline component and optionally altered by the developer.\n",
    "* (Optional) transform graph produced by an upstream Transform component.\n",
    "* (Optional) pre-trained models used for scenarios such as warmstart.\n",
    "* (Optional) hyperparameters, which will be passed to user module function. Details of the integration with Tuner can be found here.\n",
    "\n",
    "Trainer emits: At least one model for inference/serving (typically in SavedModelFormat) and optionally another model for eval (typically an EvalSavedModel).\n",
    "\n",
    "More details: https://www.tensorflow.org/tfx/guide/trainer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer = Trainer(module_file=_trainer_module_file,\r\n",
    "                  examples=transform.outputs['transformed_examples'],\r\n",
    "                  transform_graph=transform.outputs['transform_graph'],\r\n",
    "                  schema=schema_gen.outputs['schema'],\r\n",
    "                  train_args=trainer_pb2.TrainArgs(num_steps=160),\r\n",
    "                  eval_args=trainer_pb2.EvalArgs(num_steps=4),\r\n",
    "                  custom_config={'labels_path': _labels_path})\r\n",
    "context.run(trainer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_artifact_dir = trainer.outputs['model'].get()[0].uri\n",
    "print(os.listdir(model_artifact_dir))\n",
    "model_dir = os.path.join(model_artifact_dir, 'Format-Serving')\n",
    "print(os.listdir(model_dir))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {model_run_artifact_dir}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pusher Component\n",
    "\n",
    "The Pusher component is used to push a validated model to a deployment target during model training or re-training. Before the deployment, Pusher relies on one or more blessings from other validation components to decide whether to push the model or not.\n",
    "\n",
    "A Pusher component consumes a trained model in SavedModel format, and produces the same SavedModel, along with versioning metadata.\n",
    "\n",
    "More details: https://www.tensorflow.org/tfx/guide/pusher"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pusher = Pusher(model=trainer.outputs['model'],\n",
    "                push_destination=pusher_pb2.PushDestination(\n",
    "                    filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                        base_directory=_serving_model_dir)))\n",
    "context.run(pusher)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFX Pipeline Orchestration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TFX Pipeline Stitching"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     transform_module_file: str, trainer_module_file: str,\n",
    "                     serving_model_dir: str, metadata_path: str,\n",
    "                     labels_path: str) -> pipeline.Pipeline:\n",
    "    \"\"\"Implements the CIFAR10 image classification pipeline using TFX.\"\"\"\n",
    "    # This is needed for datasets with pre-defined splits\n",
    "    # Change the pattern argument to train_whole/* and test_whole/* to train\n",
    "    # on the whole CIFAR-10 dataset\n",
    "    input_config = example_gen_pb2.Input(splits=[\n",
    "        example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
    "        example_gen_pb2.Input.Split(name='eval', pattern='test/*')\n",
    "    ])\n",
    "\n",
    "    # Brings data into the pipeline.\n",
    "    example_gen = ImportExampleGen(input_base=data_root,\n",
    "                                   input_config=input_config)\n",
    "\n",
    "    # Computes statistics over data for visualization and example validation.\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "\n",
    "    # Generates schema based on statistics files.\n",
    "    schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'],\n",
    "                           infer_feature_shape=True)\n",
    "\n",
    "    # Performs transformations and feature engineering in training and serving.\n",
    "    transform = Transform(examples=example_gen.outputs['examples'],\n",
    "                          schema=schema_gen.outputs['schema'],\n",
    "                          module_file=transform_module_file)\n",
    "\n",
    "    # Uses user-provided Python function that trains a model.\n",
    "    # When traning on the whole dataset, use 18744 for train steps, 156 for eval\n",
    "    # steps. 18744 train steps correspond to 24 epochs on the whole train set, and\n",
    "    # 156 eval steps correspond to 1 epoch on the whole test set. The\n",
    "    # configuration below is for training on the dataset we provided in the data\n",
    "    # folder, which has 128 train and 128 test samples. The 160 train steps\n",
    "    # correspond to 40 epochs on this tiny train set, and 4 eval steps correspond\n",
    "    # to 1 epoch on this tiny test set.\n",
    "    trainer = Trainer(module_file=trainer_module_file,\n",
    "                      examples=transform.outputs['transformed_examples'],\n",
    "                      transform_graph=transform.outputs['transform_graph'],\n",
    "                      schema=schema_gen.outputs['schema'],\n",
    "                      train_args=trainer_pb2.TrainArgs(num_steps=160),\n",
    "                      eval_args=trainer_pb2.EvalArgs(num_steps=4),\n",
    "                      custom_config={'labels_path': labels_path})\n",
    "\n",
    "    # Checks whether the model passed the validation steps and pushes the model\n",
    "    # to a file destination if check passed.\n",
    "    pusher = Pusher(model=trainer.outputs['model'],\n",
    "                    push_destination=pusher_pb2.PushDestination(\n",
    "                        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                            base_directory=serving_model_dir)))\n",
    "\n",
    "    components = [\n",
    "        example_gen, statistics_gen, schema_gen, transform, trainer, pusher\n",
    "    ]\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TFX Orchestration "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Local Dag Runner"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=_pipeline_name,\n",
    "      pipeline_root=_pipeline_root,\n",
    "      data_root=_data_root,\n",
    "      transform_module_file=_transform_module_file,\n",
    "      trainer_module_file=_trainer_module_file,\n",
    "      serving_model_dir=_serving_model_dir,\n",
    "      metadata_path=_metadata_path,\n",
    "      labels_path=_labels_path))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [Optional] Airflow Dag Runner"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Deployment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flask"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-Serving"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "The various code snippets were borrowed from multiple"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tfxenv': conda)"
  },
  "interpreter": {
   "hash": "6d468f8d4b3a8fa99f0b8b2a7eeb41d1412ca7b959e796332de6fc9267d63a5a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}